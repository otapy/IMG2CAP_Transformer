{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQxXgu5iocwT"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from pycocotools.coco import COCO\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L83tKmP9o0yG"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data/\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data/\n",
        "\n",
        "!unzip ./data/captions_train-val2014.zip -d ./data/\n",
        "!rm ./data/captions_train-val2014.zip\n",
        "!unzip ./data/train2014.zip -d ./data/\n",
        "!rm ./data/train2014.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbmg8R6wovsx"
      },
      "outputs": [],
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def make_vocab(json, threshold):\n",
        "    coco = COCO(json)\n",
        "    counter = Counter()\n",
        "    ids = coco.anns.keys()\n",
        "    for i, id in enumerate(ids):\n",
        "        caption = str(coco.anns[id]['caption'])\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(\"[{}/{}] Tokenized captions.\".format(i+1, len(ids)))\n",
        "\n",
        "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "    return vocab\n",
        "\n",
        "vocab = make_vocab(json='./data/annotations/captions_train2014.json', threshold=4)\n",
        "vocab_path = './data/vocab.pkl'\n",
        "with open(vocab_path, 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "print(\"Saved vocabulary wrapper to '{}'\".format(vocab_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCNgyDpgo1zi"
      },
      "outputs": [],
      "source": [
        "def resize_image(image, size):\n",
        "    return image.resize((size,size), Image.ANTIALIAS)\n",
        "\n",
        "def resize_images(image_dir, output_dir, size):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    images = os.listdir(image_dir)\n",
        "    num_images = len(images)\n",
        "    for i, image in enumerate(images):\n",
        "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
        "            with Image.open(f) as img:\n",
        "                img = resize_image(img, size)\n",
        "                img.save(os.path.join(output_dir, image), img.format)\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
        "                   .format(i+1, num_images, output_dir))\n",
        "\n",
        "image_dir = './data/train2014/'\n",
        "output_dir = './data/resized2014/'\n",
        "image_size = 256\n",
        "resize_images(image_dir, output_dir, image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PJG5ih-o6m2"
      },
      "outputs": [],
      "source": [
        "class CocoDataset(data.Dataset):\n",
        "    def __init__(self, root, json, vocab, transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(json)\n",
        "        self.ids = list(self.coco.anns.keys())\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        vocab = self.vocab\n",
        "        ann_id = self.ids[index]\n",
        "        caption = coco.anns[ann_id]['caption']\n",
        "        img_id = coco.anns[ann_id]['image_id']\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "        caption = []\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyygX3S9o-Sa"
      },
      "outputs": [],
      "source": [
        "#バッチ\n",
        "def collate_fn_transformer(data):\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    images = torch.stack(images, 0)\n",
        "    img_embed_size = 65\n",
        "\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    targets = torch.zeros(len(captions), img_embed_size).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]        \n",
        "    return images, targets\n",
        "\n",
        "def get_loader_transformer(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
        "    coco = CocoDataset(root=root,\n",
        "                       json=json,\n",
        "                       vocab=vocab,\n",
        "                       transform=transform)\n",
        "    \n",
        "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=shuffle,\n",
        "                                              num_workers=num_workers,\n",
        "                                              collate_fn=collate_fn_transformer,\n",
        "                                              drop_last=True)\n",
        "    \n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOr2q2aYo_Om"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, dropout=0.1, maxlength=64):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        posemb = torch.zeros(maxlength, embed_size)\n",
        "        pos = torch.arange(0, maxlength, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(-torch.arange(0, embed_size, 2).float() * (math.log(10000.0) / embed_size))\n",
        "        posemb[:, 0::2] = torch.sin(pos * div)\n",
        "        posemb[:, 1::2] = torch.cos(pos * div)\n",
        "        posemb = posemb.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('posemb', posemb)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = math.sqrt(self.embed_size)*x + self.posemb[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKSu_B4gpE9N"
      },
      "outputs": [],
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "  def __init__(self, embed_size, img_embed_size=64, num_head=4):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.conv = nn.Conv2d(2048, embed_size, kernel_size=(1, 1), stride=(1, 1), bias=True)\n",
        "  def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = self.conv(features)\n",
        "        features = F.adaptive_avg_pool2d(features, (8, 8))\n",
        "        B, C, H, W = features.shape\n",
        "        features = features.reshape(B, C, H*W)\n",
        "        features = torch.permute(features, (0, 2, 1))\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11ULrWoUpFy5"
      },
      "outputs": [],
      "source": [
        "class Img2capTransformer(nn.Module):\n",
        "  def __init__(self, num_encoder_layers, num_decoder_layers, embed_size, vocab_size, dim_ffn=256, dropout=0.1, num_head=4):\n",
        "        super(Img2capTransformer, self).__init__()\n",
        "        #PositionEncode\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, dropout)\n",
        "        #Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_size, num_head, dim_ffn, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        #Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(embed_size, num_head, dim_ffn, batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "        #Output\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "  def forward(self, captions, features, mask_src=None, mask_tgt=None, mask_memory=None, padding_mask_src=None, padding_mask_tgt=None, memory_key_padding_mask=None):\n",
        "        #PotitionEncode\n",
        "        images_src = self.positional_encoding(features)\n",
        "        captions_tgt = self.positional_encoding(self.embed(captions))\n",
        "        #Encoder\n",
        "        features = self.transformer_encoder(images_src, mask_src, padding_mask_src)\n",
        "        #Decoder\n",
        "        captions_out = self.transformer_decoder(captions_tgt, features, mask_tgt, mask_memory, padding_mask_tgt, memory_key_padding_mask)\n",
        "        #Output\n",
        "        outputs = self.linear(captions_out)\n",
        "        return outputs\n",
        "\n",
        "  def encode(self, features, mask_src=None, padding_mask_src=None):\n",
        "        return self.transformer_encoder(self.positional_encoding(features), mask_src, padding_mask_src)\n",
        "\n",
        "  def decode(self, captions, features, mask_tgt=None, mask_memory=None, padding_mask_tgt=None, memory_key_padding_mask=None):\n",
        "        return self.transformer_decoder(self.positional_encoding(self.embed(captions)), features, mask_tgt, mask_memory, padding_mask_tgt, memory_key_padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlmZBgRJpRUD"
      },
      "outputs": [],
      "source": [
        "#学習\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = 'transformermodels/'\n",
        "crop_size = 224\n",
        "vocab_path = 'data/vocab.pkl'\n",
        "image_dir ='data/resized2014'\n",
        "caption_path='data/annotations/captions_train2014.json'\n",
        "log_step=10\n",
        "save_step=6000\n",
        "embed_size=512\n",
        "num_layers=1\n",
        "num_epochs=30\n",
        "batch_size=64\n",
        "num_workers=2\n",
        "learning_rate=0.001\n",
        "num_encoder_layers=4\n",
        "num_decoder_layers=4\n",
        "dim_ffn=512\n",
        "dropout=0.1\n",
        "num_head=8\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(crop_size),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "data_loader = get_loader_transformer(image_dir, caption_path, vocab, transform, batch_size,\n",
        "                          shuffle=True, num_workers=num_workers) \n",
        "\n",
        "cnnencoder = CNNEncoder(embed_size=embed_size).to(device)\n",
        "transformer = Img2capTransformer(num_encoder_layers, num_decoder_layers, embed_size, len(vocab), dim_ffn=dim_ffn, dropout=dropout, num_head=num_head).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(cnnencoder.parameters()) + list(transformer.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
        "\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, captions) in enumerate(data_loader):\n",
        "        \n",
        "        #torch.autograd.set_detect_anomaly(True)\n",
        "        images = images.to(device)\n",
        "\n",
        "        caption = captions[:, :-1]\n",
        "        targets = captions[:, 1:]\n",
        "\n",
        "        caption = caption.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        feature = cnnencoder(images)\n",
        "        \n",
        "        pad = 0\n",
        "        src_mask = torch.zeros((num_head*batch_size, feature.size(1), feature.size(1)), dtype=bool).to(device)\n",
        "        tgt_mask = torch.triu(torch.full((num_head*batch_size, targets.size(1), targets.size(1)), float('-inf')), diagonal=1).to(device)\n",
        "        memory_mask = torch.zeros((num_head*batch_size, feature.size(1), feature.size(1)), dtype=bool).to(device)\n",
        "        src_key_padding_mask = (feature == pad).to(device)\n",
        "        tgt_key_padding_mask = (targets == pad).to(device)\n",
        "        memory_key_padding_mask = (feature == pad).to(device)\n",
        "        \n",
        "        outputs = transformer(caption, feature, mask_src=None, mask_tgt=tgt_mask, mask_memory=None, padding_mask_src=None, padding_mask_tgt=tgt_key_padding_mask, memory_key_padding_mask=None)\n",
        "\n",
        "        outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        cnnencoder.zero_grad()\n",
        "        transformer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % log_step == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
        "            \n",
        "        if (i+1) % save_step == 0:\n",
        "            torch.save(cnnencoder.state_dict(), os.path.join(\n",
        "                model_path, 'cnnencoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "            torch.save(transformer.state_dict(), os.path.join(\n",
        "                model_path, 'transformer-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "            \n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoNDuS5-v7rU"
      },
      "outputs": [],
      "source": [
        "#予測\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = 'transformermodels/'\n",
        "crop_size = 224\n",
        "vocab_path = 'data/vocab.pkl'\n",
        "image_dir ='data/resized2014'\n",
        "caption_path='data/annotations/captions_train2014.json'\n",
        "embed_size=256\n",
        "num_epochs=30\n",
        "num_encoder_layers=4\n",
        "num_decoder_layers=4\n",
        "dim_ffn=512\n",
        "dropout=0.1\n",
        "num_head=8\n",
        "\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "cnnencoder = CNNEncoder(embed_size=embed_size).to(device)\n",
        "transformer = Img2capTransformer(num_encoder_layers, num_decoder_layers, embed_size, len(vocab), dim_ffn=dim_ffn, dropout=dropout, num_head=num_head).to(device)\n",
        "cnnencoder.load_state_dict(torch.load('./transformermodels/cnnencoder-1-6000.ckpt',torch.device('cpu')))\n",
        "transformer.load_state_dict(torch.load('./transformermodels/transformer-1-6000.ckpt',torch.device('cpu')))\n",
        "cnnencoder.eval()\n",
        "transformer.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_img_dir = './data/train2014/COCO_train2014_000000000138.jpg'\n",
        "test_img = Image.open(test_img_dir).convert('RGB')\n",
        "test_img = test_img.resize([256, 256], Image.LANCZOS)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "test_img = transform(test_img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#予測\n",
        "def predict(images, max_length, states=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    predicted_ids = []\n",
        "    caption = []\n",
        "    start_char = [vocab('<start>')]\n",
        "    input_char = torch.tensor(start_char, device=device)\n",
        "    input_char = input_char.unsqueeze(1)\n",
        "    images = images.unsqueeze(0).to(device)\n",
        "    feature = cnnencoder(images).to(device)\n",
        "    memory = transformer.encode(feature).to(device)\n",
        "    for i in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            pad = 0\n",
        "            tgt_mask = torch.triu(torch.full((8*input_char.size(0), input_char.size(1), input_char.size(1)), float('-inf')), diagonal=1).to(device)\n",
        "            tgt_key_padding_mask = (input_char == pad).to(device)\n",
        "            outputs = transformer.decode(input_char, memory, mask_tgt=tgt_mask, padding_mask_tgt=tgt_key_padding_mask).to(device)\n",
        "            outputs = transformer.linear(outputs[:]).to(device)\n",
        "            _, output_chars = torch.max(outputs,dim=2)\n",
        "            if int(output_chars[:,-1]) == vocab('<end>'):\n",
        "                break\n",
        "            output_char = output_chars[:,-1]\n",
        "            predicted_ids.append(output_char)\n",
        "            input_char = torch.cat([torch.tensor(start_char, device=device).unsqueeze(0), output_chars], dim=1)\n",
        "    predicted_ids = torch.stack(predicted_ids, 1)\n",
        "    predicted_ids = predicted_ids[0]\n",
        "    for j in range(len(predicted_ids)):\n",
        "        word = vocab.idx2word[int(predicted_ids[j])]\n",
        "        caption.append(word)\n",
        "        sentence = ' '.join(caption[:])\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "rkMozUBIP98M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}